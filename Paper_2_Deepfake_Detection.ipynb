{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMb24XyEHW65",
        "outputId": "827ed7ec-d2ee-4688-ed32-5c6d8bbe0a79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Checking if all images are 256X256 with 3 RGB channels\n",
        "</div>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-GkDV2hiLuFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/gan_datasets/biggan/0_real'\n",
        "images_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "print(len(images_list))\n",
        "# for file_name in images_list:\n",
        "#     file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "#     if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "\n",
        "#         image = cv2.imread(file_path)\n",
        "#         height, width, channels = image.shape\n",
        "#         if(height!=256 or width!=256 or channels!=3):\n",
        "#            print(\"Dimensions/channels mismatch for \",file_name)\n",
        "#     else:\n",
        "#         print(file_name,\" is not an image.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDSalAnlgND1",
        "outputId": "328cb924-bc00-4be7-fe14-c6ee7fa2d3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## <h1>**Level 1 of the Multilevel hierarchichal architecture**</h1>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0QJ0K7SlIE_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Creating separate training and validation directories with real(imagenet) and fake(dalle) subfolders under each\n",
        "</div>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ybn648p4hSFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    lvl1_datasets\n",
        "    ├── training\n",
        "    │   ├── 0_real\n",
        "    │   └── 1_fake\n",
        "    └── validation\n",
        "        ├── 0_real\n",
        "        └── 1_fake\n"
      ],
      "metadata": {
        "id": "b2anOkw2ITq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "\n",
        "real_dir=\"/content/drive/MyDrive/diffusion_datasets/laion/0_real\"\n",
        "fake_dir=\"/content/drive/MyDrive/gan_datasets/biggan/1_fake\"\n",
        "\n",
        "os.makedirs(\"/content/drive/MyDrive/lvl1_biggan_laion_datasets\", exist_ok=True)\n",
        "\n",
        "\n",
        "training_dir = '/content/drive/MyDrive/lvl1_biggan_laion_datasets/training_dataset'\n",
        "validation_dir = '/content/drive/MyDrive/lvl1_biggan_laion_datasets/validation_dataset'\n",
        "os.makedirs(training_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "real_images = [os.path.join(real_dir, filename) for filename in os.listdir(real_dir)][:1000]\n",
        "fake_images = [os.path.join(fake_dir, filename) for filename in os.listdir(fake_dir)][:1000]\n",
        "\n",
        "\n",
        "all_images = real_images + fake_images\n",
        "train_images, val_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "for image_path in train_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(training_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)\n",
        "\n",
        "for image_path in val_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(validation_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)"
      ],
      "metadata": {
        "id": "o_QHSopobPMx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Creating real and fake subfolders for testing\n",
        "</div>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Moa_5nVQ7jEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    testing_datasets\n",
        "      ├── 0_real\n",
        "      └── 1_fake\n",
        "     "
      ],
      "metadata": {
        "id": "h-XYSwi4KqbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "\n",
        "real_dir=\"/content/drive/MyDrive/gan_datasets/crn_subset/0_real\"\n",
        "fake_dir=\"/content/drive/MyDrive/gan_datasets/crn_subset/1_fake\"\n",
        "test_dir = '/content/drive/MyDrive/testing_lvl1_crn'\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "real_images = [os.path.join(real_dir, filename) for filename in os.listdir(real_dir)]\n",
        "fake_images = [os.path.join(fake_dir, filename) for filename in os.listdir(fake_dir)]\n",
        "\n",
        "\n",
        "for image_path in real_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(test_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)\n",
        "\n",
        "for image_path in fake_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(test_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)"
      ],
      "metadata": {
        "id": "2C5UriIW7ikl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Loading both train and validation data\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "Mfxkeic3z6I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/lvl1_biggan_laion_datasets/training_dataset'\n",
        "validation_dir = '/content/drive/MyDrive/lvl1_biggan_laion_datasets/validation_dataset'\n",
        "\n",
        "# def apply_gaussian_blur(image):\n",
        "#     image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "#     return image\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    # preprocessing_function=apply_gaussian_blur\n",
        "    )\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M7ycn22nPl1",
        "outputId": "05c9cc90-47c6-4bd2-a6c8-bae50805ff75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1600 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><strong> Overall structure</strong></h2>\n",
        "\n",
        "1. **Datasets**:\n",
        "Training dataset: Laion and Biggan generated 800 images each <br>\n",
        "Validation dataset: Laion and Biggan generated 200 images each\n",
        "\n",
        "2. **Data Preprocessing**:\n",
        "Rescaled pixel values to [0, 1]<br>\n",
        "Used ImageDataGenerator to create image generators which one hot <br>encodes the traiing labels with batch size 30 and binary class mode\n",
        "\n",
        "3. **Base Model**:\n",
        "ResNet50 pre-trained on ImageNet<br>\n",
        "Top layers excluded<br>\n",
        "Input shape set to (256, 256, 3)\n",
        "\n",
        "4. **Model Architecture**:\n",
        "Sequential model with:<br>\n",
        "Base ResNet50 model<br>\n",
        "GlobalAveragePooling2D layer<br>\n",
        "Dense layer with 1 neuron and sigmoid activation\n",
        "\n",
        "5. **Model Compilation**:\n",
        "Optimizer: SGD with learning rate 0.001 and momentum 0.9<br>\n",
        "Loss function: binary_crossentropy<br>\n",
        "Metrics: accuracy\n",
        "\n",
        "6. **Training Specifications**:\n",
        "Initial epochs: 30<br>\n",
        "Batch size: 30<br>\n",
        "Fine-tuning : 5 epochs<br>\n",
        "Unfreezed layers: 130-175\n",
        "\n",
        "7. **Hyperparameters**:\n",
        "Learning rate: 0.001<br>\n",
        "Momentum: 0.9<br>\n",
        "Batch size: 30<br>\n",
        "Total epochs : 35"
      ],
      "metadata": {
        "id": "KHA5qNeYt1xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Building model using pretrained weights from ResNet50 avaliable and fitting it initially without fine tuning it\n",
        "</div>"
      ],
      "metadata": {
        "id": "OTDCxRwKAkqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_1 = ResNet50(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "model = Sequential([\n",
        "    base_model_1,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "for layer in base_model_1.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "initial_epochs = 30\n",
        "fine_tune_epochs = 5\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "\n",
        "history = model.fit(train_generator, epochs=initial_epochs,\n",
        "                    validation_data=validation_generator)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dim1HLFGAKUD",
        "outputId": "14c8f584-f2d8-42f2-fd8a-744095fba123"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 3s 0us/step\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 640s 12s/step - loss: 0.7120 - accuracy: 0.5144 - val_loss: 0.6841 - val_accuracy: 0.5250\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 11s 196ms/step - loss: 0.7087 - accuracy: 0.5331 - val_loss: 0.7277 - val_accuracy: 0.4975\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 11s 193ms/step - loss: 0.6839 - accuracy: 0.5481 - val_loss: 0.6603 - val_accuracy: 0.6275\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 11s 207ms/step - loss: 0.6522 - accuracy: 0.5944 - val_loss: 0.7010 - val_accuracy: 0.5300\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 11s 209ms/step - loss: 0.6367 - accuracy: 0.6325 - val_loss: 0.6460 - val_accuracy: 0.6550\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 11s 196ms/step - loss: 0.6289 - accuracy: 0.6338 - val_loss: 0.6443 - val_accuracy: 0.6225\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 11s 208ms/step - loss: 0.6145 - accuracy: 0.6675 - val_loss: 0.6321 - val_accuracy: 0.6850\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 11s 204ms/step - loss: 0.6061 - accuracy: 0.6681 - val_loss: 0.6646 - val_accuracy: 0.5725\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 11s 202ms/step - loss: 0.6197 - accuracy: 0.6381 - val_loss: 0.7150 - val_accuracy: 0.5550\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 11s 205ms/step - loss: 0.6145 - accuracy: 0.6594 - val_loss: 0.6164 - val_accuracy: 0.6975\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 11s 196ms/step - loss: 0.5897 - accuracy: 0.6981 - val_loss: 0.6225 - val_accuracy: 0.7050\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 11s 198ms/step - loss: 0.5900 - accuracy: 0.6869 - val_loss: 0.6329 - val_accuracy: 0.6025\n",
            "Epoch 13/30\n",
            "54/54 [==============================] - 11s 203ms/step - loss: 0.5997 - accuracy: 0.6756 - val_loss: 0.7364 - val_accuracy: 0.4975\n",
            "Epoch 14/30\n",
            "54/54 [==============================] - 11s 203ms/step - loss: 0.5999 - accuracy: 0.6712 - val_loss: 0.7406 - val_accuracy: 0.4975\n",
            "Epoch 15/30\n",
            "54/54 [==============================] - 11s 204ms/step - loss: 0.5985 - accuracy: 0.6562 - val_loss: 0.6000 - val_accuracy: 0.7100\n",
            "Epoch 16/30\n",
            "54/54 [==============================] - 11s 205ms/step - loss: 0.5795 - accuracy: 0.7100 - val_loss: 0.6115 - val_accuracy: 0.6375\n",
            "Epoch 17/30\n",
            "54/54 [==============================] - 11s 204ms/step - loss: 0.5730 - accuracy: 0.7156 - val_loss: 0.5957 - val_accuracy: 0.6975\n",
            "Epoch 18/30\n",
            "54/54 [==============================] - 11s 208ms/step - loss: 0.5774 - accuracy: 0.7031 - val_loss: 0.6468 - val_accuracy: 0.5925\n",
            "Epoch 19/30\n",
            "54/54 [==============================] - 11s 209ms/step - loss: 0.6005 - accuracy: 0.6737 - val_loss: 0.6577 - val_accuracy: 0.5650\n",
            "Epoch 20/30\n",
            "54/54 [==============================] - 12s 224ms/step - loss: 0.5857 - accuracy: 0.6794 - val_loss: 0.5898 - val_accuracy: 0.7125\n",
            "Epoch 21/30\n",
            "54/54 [==============================] - 14s 262ms/step - loss: 0.5867 - accuracy: 0.6675 - val_loss: 0.6233 - val_accuracy: 0.6175\n",
            "Epoch 22/30\n",
            "54/54 [==============================] - 13s 237ms/step - loss: 0.5620 - accuracy: 0.7219 - val_loss: 0.5859 - val_accuracy: 0.7050\n",
            "Epoch 23/30\n",
            "54/54 [==============================] - 13s 245ms/step - loss: 0.5567 - accuracy: 0.7125 - val_loss: 0.5846 - val_accuracy: 0.7150\n",
            "Epoch 24/30\n",
            "54/54 [==============================] - 12s 226ms/step - loss: 0.5729 - accuracy: 0.6963 - val_loss: 0.6017 - val_accuracy: 0.6450\n",
            "Epoch 25/30\n",
            "54/54 [==============================] - 12s 214ms/step - loss: 0.5744 - accuracy: 0.6888 - val_loss: 0.5812 - val_accuracy: 0.7175\n",
            "Epoch 26/30\n",
            "54/54 [==============================] - 12s 227ms/step - loss: 0.5515 - accuracy: 0.7306 - val_loss: 0.5801 - val_accuracy: 0.7150\n",
            "Epoch 27/30\n",
            "54/54 [==============================] - 11s 202ms/step - loss: 0.5582 - accuracy: 0.7138 - val_loss: 0.5794 - val_accuracy: 0.7000\n",
            "Epoch 28/30\n",
            "54/54 [==============================] - 10s 189ms/step - loss: 0.5489 - accuracy: 0.7144 - val_loss: 0.6211 - val_accuracy: 0.6225\n",
            "Epoch 29/30\n",
            "54/54 [==============================] - 11s 206ms/step - loss: 0.5531 - accuracy: 0.7212 - val_loss: 0.5765 - val_accuracy: 0.7075\n",
            "Epoch 30/30\n",
            "54/54 [==============================] - 12s 219ms/step - loss: 0.5453 - accuracy: 0.7250 - val_loss: 0.5755 - val_accuracy: 0.7150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Fine-tuning the model for 5 epochs\n",
        "</div>"
      ],
      "metadata": {
        "id": "GghsxKFzAw_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_layers=130\n",
        "\n",
        "for layer in model.layers[fine_tune_layers:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(train_generator, epochs=total_epochs,\n",
        "                         initial_epoch=initial_epochs,\n",
        "                         validation_data=validation_generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbUN_eVn_Dx_",
        "outputId": "0f53fe4b-3a95-455c-d0e8-eea47b3c3ab3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/35\n",
            "54/54 [==============================] - 15s 219ms/step - loss: 0.5483 - accuracy: 0.7269 - val_loss: 0.5817 - val_accuracy: 0.6700\n",
            "Epoch 32/35\n",
            "54/54 [==============================] - 11s 199ms/step - loss: 0.5415 - accuracy: 0.7275 - val_loss: 0.5723 - val_accuracy: 0.7275\n",
            "Epoch 33/35\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 0.5489 - accuracy: 0.7119 - val_loss: 0.5711 - val_accuracy: 0.7275\n",
            "Epoch 34/35\n",
            "54/54 [==============================] - 11s 206ms/step - loss: 0.5368 - accuracy: 0.7362 - val_loss: 0.5727 - val_accuracy: 0.7000\n",
            "Epoch 35/35\n",
            "54/54 [==============================] - 11s 197ms/step - loss: 0.5516 - accuracy: 0.7125 - val_loss: 0.5733 - val_accuracy: 0.7325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Saving the model\n",
        "</div>"
      ],
      "metadata": {
        "id": "1aAccRz8PokS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/saved_models_deepfake',exist_ok=True)\n",
        "model.save('/content/drive/MyDrive/saved_models_deepfake/lvl1_laion_biggan.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-5qGn0qGns3",
        "outputId": "c9b0782a-23b9-4ba6-c35f-0f8fef0caebd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Testing the model for level 1 classification(real vs fake)\n",
        "</div>"
      ],
      "metadata": {
        "id": "EMWg_aVkBGvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/saved_models_deepfake/lvl1_laion_biggan.h5')\n",
        "\n",
        "test_dir = '/content/drive/MyDrive/testing_lvl1_crn'\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "_, test_accuracy = loaded_model.evaluate(test_generator)\n",
        "\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_l7irkS-_sa",
        "outputId": "dff12a11-3ab4-4f33-e699-c87456a9453f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1996 images belonging to 2 classes.\n",
            "67/67 [==============================] - 531s 8s/step - loss: 0.6956 - accuracy: 0.5421\n",
            "Test accuracy: 0.5420841574668884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## <h1>**Level 2 of the Multilevel hierarchichal architecture**</h1>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w6I-UIl9NyO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Making directories for level 2 datasets similiarly\n",
        "</div>"
      ],
      "metadata": {
        "id": "z-pSGBXeXfuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "\n",
        "dms_dir=\"/content/drive/MyDrive/diffusion_datasets/dalle/0_dm\"\n",
        "gans_dir=\"/content/drive/MyDrive/gan_datasets/biggan/1_gan\"\n",
        "\n",
        "os.makedirs(\"/content/drive/MyDrive/lvl2_biggan_dalle_datasets\",exist_ok=True)\n",
        "\n",
        "training_dir = '/content/drive/MyDrive/lvl2_biggan_dalle_datasets/training_dataset'\n",
        "validation_dir = '/content/drive/MyDrive/lvl2_biggan_dalle_datasets/validation_dataset'\n",
        "os.makedirs(training_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "dms_images = [os.path.join(dms_dir, filename) for filename in os.listdir(dms_dir)][:1000]\n",
        "gans_images = [os.path.join(gans_dir, filename) for filename in os.listdir(gans_dir)][:1000]\n",
        "\n",
        "all_images = dms_images + gans_images\n",
        "train_images, val_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "\n",
        "for image_path in train_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(training_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)\n",
        "\n",
        "for image_path in val_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(validation_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)"
      ],
      "metadata": {
        "id": "O7bfVPChCUY8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## ResNet implementation with linear layer at end for level 2 datasets similiarly\n",
        "</div>"
      ],
      "metadata": {
        "id": "a_kt1pt6RXyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import cv2\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/lvl2_biggan_dalle_datasets/training_dataset'\n",
        "validation_dir = '/content/drive/MyDrive/lvl2_biggan_dalle_datasets/validation_dataset'\n",
        "\n",
        "# def apply_gaussian_blur(image):\n",
        "#     image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "#     return image\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "base_model_2 = ResNet50(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "model = Sequential([\n",
        "    base_model_2,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "for layer in base_model_2.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "initial_epochs = 10\n",
        "fine_tune_epochs = 15\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "\n",
        "history = model.fit(train_generator, epochs=initial_epochs,\n",
        "                    validation_data=validation_generator)\n",
        "\n",
        "\n",
        "\n",
        "fine_tune_layers=140\n",
        "\n",
        "for layer in model.layers[fine_tune_layers:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(train_generator, epochs=total_epochs,\n",
        "                         initial_epoch=initial_epochs,\n",
        "                         validation_data=validation_generator)\n",
        "\n",
        "model.save('/content/drive/MyDrive/saved_models_deepfake/lvl2_biggan_dalle.h5')"
      ],
      "metadata": {
        "id": "o0Jfc8ZWHO_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed47f1c1-0ba4-4f48-e064-4af81d1d89e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1600 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 304s 6s/step - loss: 0.6973 - accuracy: 0.5369 - val_loss: 0.6931 - val_accuracy: 0.4975\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 12s 223ms/step - loss: 0.6734 - accuracy: 0.5781 - val_loss: 0.6562 - val_accuracy: 0.5575\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 13s 235ms/step - loss: 0.6312 - accuracy: 0.6431 - val_loss: 0.6225 - val_accuracy: 0.6500\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 12s 231ms/step - loss: 0.6182 - accuracy: 0.6431 - val_loss: 0.6050 - val_accuracy: 0.6700\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 13s 230ms/step - loss: 0.6222 - accuracy: 0.6369 - val_loss: 0.6142 - val_accuracy: 0.6600\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 12s 220ms/step - loss: 0.6084 - accuracy: 0.6594 - val_loss: 0.5866 - val_accuracy: 0.6925\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 12s 223ms/step - loss: 0.6309 - accuracy: 0.6319 - val_loss: 0.5807 - val_accuracy: 0.7050\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 13s 230ms/step - loss: 0.6102 - accuracy: 0.6450 - val_loss: 0.6065 - val_accuracy: 0.6750\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 12s 226ms/step - loss: 0.5971 - accuracy: 0.6731 - val_loss: 0.5700 - val_accuracy: 0.7075\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 13s 237ms/step - loss: 0.5910 - accuracy: 0.6750 - val_loss: 0.6171 - val_accuracy: 0.6200\n",
            "Epoch 11/25\n",
            "54/54 [==============================] - 18s 247ms/step - loss: 0.6197 - accuracy: 0.6469 - val_loss: 0.6327 - val_accuracy: 0.6400\n",
            "Epoch 12/25\n",
            "54/54 [==============================] - 12s 223ms/step - loss: 0.5866 - accuracy: 0.6875 - val_loss: 0.5649 - val_accuracy: 0.7225\n",
            "Epoch 13/25\n",
            "54/54 [==============================] - 13s 233ms/step - loss: 0.5838 - accuracy: 0.7019 - val_loss: 0.5843 - val_accuracy: 0.7025\n",
            "Epoch 14/25\n",
            "54/54 [==============================] - 12s 225ms/step - loss: 0.6044 - accuracy: 0.6556 - val_loss: 0.5573 - val_accuracy: 0.7275\n",
            "Epoch 15/25\n",
            "54/54 [==============================] - 13s 238ms/step - loss: 0.5775 - accuracy: 0.6900 - val_loss: 0.5522 - val_accuracy: 0.7100\n",
            "Epoch 16/25\n",
            "54/54 [==============================] - 15s 276ms/step - loss: 0.5766 - accuracy: 0.6981 - val_loss: 0.5634 - val_accuracy: 0.6900\n",
            "Epoch 17/25\n",
            "54/54 [==============================] - 13s 234ms/step - loss: 0.5776 - accuracy: 0.6881 - val_loss: 0.5715 - val_accuracy: 0.6850\n",
            "Epoch 18/25\n",
            "54/54 [==============================] - 13s 230ms/step - loss: 0.5783 - accuracy: 0.6900 - val_loss: 0.5849 - val_accuracy: 0.6800\n",
            "Epoch 19/25\n",
            "54/54 [==============================] - 13s 235ms/step - loss: 0.5865 - accuracy: 0.6719 - val_loss: 0.5457 - val_accuracy: 0.7425\n",
            "Epoch 20/25\n",
            "54/54 [==============================] - 13s 232ms/step - loss: 0.5843 - accuracy: 0.6900 - val_loss: 0.5616 - val_accuracy: 0.7350\n",
            "Epoch 21/25\n",
            "54/54 [==============================] - 15s 278ms/step - loss: 0.5682 - accuracy: 0.6956 - val_loss: 0.5409 - val_accuracy: 0.7475\n",
            "Epoch 22/25\n",
            "54/54 [==============================] - 13s 233ms/step - loss: 0.5618 - accuracy: 0.7006 - val_loss: 0.5424 - val_accuracy: 0.7100\n",
            "Epoch 23/25\n",
            "54/54 [==============================] - 15s 269ms/step - loss: 0.5668 - accuracy: 0.6913 - val_loss: 0.5580 - val_accuracy: 0.7275\n",
            "Epoch 24/25\n",
            "54/54 [==============================] - 15s 269ms/step - loss: 0.5702 - accuracy: 0.6913 - val_loss: 0.5376 - val_accuracy: 0.7175\n",
            "Epoch 25/25\n",
            "54/54 [==============================] - 15s 277ms/step - loss: 0.5652 - accuracy: 0.7056 - val_loss: 0.5345 - val_accuracy: 0.7400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Creating testing directory with similiar structure\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "bhR9kPpEWx7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "\n",
        "gan_dir=\"/content/drive/MyDrive/gan_datasets/crn_subset/1_gan\"\n",
        "dm_dir=\"/content/drive/MyDrive/diffusion_datasets/ldm_100/0_dm\"\n",
        "test_dir = '/content/drive/MyDrive/testing_dir_lvl2_crn_ldm_100'\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "gan_images = [os.path.join(gan_dir, filename) for filename in os.listdir(gan_dir)][:750]\n",
        "dm_images = [os.path.join(dm_dir, filename) for filename in os.listdir(dm_dir)][:750]\n",
        "\n",
        "\n",
        "for image_path in gan_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(test_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)\n",
        "\n",
        "for image_path in dm_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(test_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)"
      ],
      "metadata": {
        "id": "4EwEBDn0QVW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Testing the model for level 2 classification(GANs vs DMs)\n",
        "</div>"
      ],
      "metadata": {
        "id": "hQ2M8GcIW8fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = '/content/drive/MyDrive/testing_dir_lvl2_crn_ldm_100'\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "_, test_accuracy = model.evaluate(test_generator)\n",
        "\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O8IyMJcWWG-",
        "outputId": "aa117182-cccf-4ebe-dd28-8d33e04b42f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1500 images belonging to 2 classes.\n",
            "50/50 [==============================] - 11s 212ms/step - loss: 0.6425 - accuracy: 0.6340\n",
            "Test accuracy: 0.6340000033378601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## <h1>**Level 3 of the Multilevel hierarchichal architecture**</h1>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sj-G-iiVN17g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Making directories for Level 3 of Architecture\n",
        "</div>"
      ],
      "metadata": {
        "id": "zR7IbQTR6NOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "\n",
        "dalle_dir=\"/content/drive/MyDrive/diffusion_datasets/dalle/0_dalle\"\n",
        "glide_dir=\"/content/drive/MyDrive/diffusion_datasets/glide_100_27/1_glide\"\n",
        "guided_dir=\"/content/drive/MyDrive/diffusion_datasets/guided/2_guided\"\n",
        "ldm_dir=\"/content/drive/MyDrive/diffusion_datasets/ldm_200/3_ldm\"\n",
        "\n",
        "os.makedirs(\"/content/drive/MyDrive/lvl3_dm_datasets\", exist_ok=True)\n",
        "\n",
        "\n",
        "training_dir = '/content/drive/MyDrive/lvl3_dm_datasets/training_dataset'\n",
        "validation_dir = '/content/drive/MyDrive/lvl3_dm_datasets/validation_dataset'\n",
        "os.makedirs(training_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "dalle_images = [os.path.join(dalle_dir, filename) for filename in os.listdir(dalle_dir)][:500]\n",
        "glide_images = [os.path.join(glide_dir, filename) for filename in os.listdir(glide_dir)][:500]\n",
        "guided_images = [os.path.join(guided_dir, filename) for filename in os.listdir(guided_dir)][:500]\n",
        "ldm_images = [os.path.join(ldm_dir, filename) for filename in os.listdir(ldm_dir)][:500]\n",
        "\n",
        "all_images = dalle_images + glide_images + guided_images + ldm_images\n",
        "train_images, val_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "for image_path in train_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(training_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)\n",
        "\n",
        "for image_path in val_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(validation_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)"
      ],
      "metadata": {
        "id": "QjQw5yMiBFwR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Implementating multiclass classification for all the diffusion models using ResNet50's pretrained weights and fine tuning them\n",
        "</div>"
      ],
      "metadata": {
        "id": "DeOojAJQ6bel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import cv2\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/lvl3_dm_datasets/training_dataset'\n",
        "validation_dir = '/content/drive/MyDrive/lvl3_dm_datasets/validation_dataset'\n",
        "\n",
        "# def apply_gaussian_blur(image):\n",
        "#     image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "#     return image\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  #  preprocessing_function=apply_gaussian_blur\n",
        "                                   )\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "base_model_3 = ResNet50(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "model = Sequential([\n",
        "    base_model_3,\n",
        "    GlobalAveragePooling2D(),\n",
        "    # Dense(64,activation='relu'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "for layer in base_model_3.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.9\n",
        ")\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "initial_epochs = 30\n",
        "fine_tune_epochs = 5\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "\n",
        "history = model.fit(train_generator, epochs=initial_epochs,\n",
        "                    validation_data=validation_generator)\n",
        "\n",
        "\n",
        "\n",
        "fine_tune_layers=140\n",
        "\n",
        "for layer in model.layers[fine_tune_layers:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(train_generator, epochs=total_epochs,\n",
        "                         initial_epoch=initial_epochs,\n",
        "                         validation_data=validation_generator)\n",
        "\n",
        "model.save('/content/drive/MyDrive/saved_models_deepfake/lvl3_dalle.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRUZJqfVsVHM",
        "outputId": "f677cc1c-7558-4b72-ee5e-ffd7ce1060b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1600 images belonging to 4 classes.\n",
            "Found 400 images belonging to 4 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 309s 6s/step - loss: 1.4410 - accuracy: 0.2738 - val_loss: 1.3188 - val_accuracy: 0.3600\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 14s 267ms/step - loss: 1.2925 - accuracy: 0.3725 - val_loss: 1.2114 - val_accuracy: 0.4475\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 13s 243ms/step - loss: 1.2405 - accuracy: 0.4094 - val_loss: 1.2224 - val_accuracy: 0.4525\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 15s 277ms/step - loss: 1.2652 - accuracy: 0.3856 - val_loss: 1.1607 - val_accuracy: 0.4875\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 16s 288ms/step - loss: 1.2633 - accuracy: 0.3931 - val_loss: 1.2306 - val_accuracy: 0.3925\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 15s 285ms/step - loss: 1.2165 - accuracy: 0.4263 - val_loss: 1.1454 - val_accuracy: 0.4700\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 14s 250ms/step - loss: 1.1974 - accuracy: 0.4313 - val_loss: 1.3377 - val_accuracy: 0.4375\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 13s 237ms/step - loss: 1.2222 - accuracy: 0.4256 - val_loss: 1.1495 - val_accuracy: 0.4325\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 13s 229ms/step - loss: 1.2457 - accuracy: 0.4181 - val_loss: 1.1424 - val_accuracy: 0.4875\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 14s 264ms/step - loss: 1.1993 - accuracy: 0.4313 - val_loss: 1.1409 - val_accuracy: 0.4550\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 13s 248ms/step - loss: 1.2127 - accuracy: 0.4512 - val_loss: 1.2402 - val_accuracy: 0.4425\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 13s 244ms/step - loss: 1.2108 - accuracy: 0.4313 - val_loss: 1.1253 - val_accuracy: 0.4900\n",
            "Epoch 13/30\n",
            "54/54 [==============================] - 12s 230ms/step - loss: 1.2280 - accuracy: 0.4231 - val_loss: 1.1835 - val_accuracy: 0.3875\n",
            "Epoch 14/30\n",
            "54/54 [==============================] - 12s 227ms/step - loss: 1.2211 - accuracy: 0.4263 - val_loss: 1.1928 - val_accuracy: 0.3850\n",
            "Epoch 15/30\n",
            "54/54 [==============================] - 15s 280ms/step - loss: 1.1949 - accuracy: 0.4556 - val_loss: 1.1342 - val_accuracy: 0.4600\n",
            "Epoch 16/30\n",
            "54/54 [==============================] - 15s 272ms/step - loss: 1.1857 - accuracy: 0.4437 - val_loss: 1.1623 - val_accuracy: 0.4975\n",
            "Epoch 17/30\n",
            "54/54 [==============================] - 13s 230ms/step - loss: 1.1871 - accuracy: 0.4631 - val_loss: 1.2197 - val_accuracy: 0.3875\n",
            "Epoch 18/30\n",
            "54/54 [==============================] - 13s 233ms/step - loss: 1.2140 - accuracy: 0.4525 - val_loss: 1.1123 - val_accuracy: 0.5025\n",
            "Epoch 19/30\n",
            "54/54 [==============================] - 13s 245ms/step - loss: 1.2507 - accuracy: 0.4412 - val_loss: 1.1156 - val_accuracy: 0.5100\n",
            "Epoch 20/30\n",
            "54/54 [==============================] - 15s 269ms/step - loss: 1.1977 - accuracy: 0.4594 - val_loss: 1.4207 - val_accuracy: 0.3900\n",
            "Epoch 21/30\n",
            "54/54 [==============================] - 19s 346ms/step - loss: 1.1993 - accuracy: 0.4556 - val_loss: 1.1543 - val_accuracy: 0.4675\n",
            "Epoch 22/30\n",
            "54/54 [==============================] - 16s 289ms/step - loss: 1.1782 - accuracy: 0.4575 - val_loss: 1.1335 - val_accuracy: 0.4925\n",
            "Epoch 23/30\n",
            "54/54 [==============================] - 13s 244ms/step - loss: 1.1697 - accuracy: 0.4825 - val_loss: 1.1975 - val_accuracy: 0.4275\n",
            "Epoch 24/30\n",
            "54/54 [==============================] - 16s 303ms/step - loss: 1.1874 - accuracy: 0.4613 - val_loss: 1.1861 - val_accuracy: 0.4525\n",
            "Epoch 25/30\n",
            "54/54 [==============================] - 13s 233ms/step - loss: 1.1852 - accuracy: 0.4750 - val_loss: 1.1104 - val_accuracy: 0.5425\n",
            "Epoch 26/30\n",
            "54/54 [==============================] - 13s 239ms/step - loss: 1.1786 - accuracy: 0.4525 - val_loss: 1.1361 - val_accuracy: 0.4775\n",
            "Epoch 27/30\n",
            "54/54 [==============================] - 13s 249ms/step - loss: 1.1706 - accuracy: 0.4812 - val_loss: 1.1132 - val_accuracy: 0.5125\n",
            "Epoch 28/30\n",
            "54/54 [==============================] - 15s 273ms/step - loss: 1.1435 - accuracy: 0.4819 - val_loss: 1.1029 - val_accuracy: 0.5175\n",
            "Epoch 29/30\n",
            "54/54 [==============================] - 13s 233ms/step - loss: 1.1699 - accuracy: 0.4650 - val_loss: 1.1268 - val_accuracy: 0.4800\n",
            "Epoch 30/30\n",
            "54/54 [==============================] - 14s 255ms/step - loss: 1.1468 - accuracy: 0.4806 - val_loss: 1.1419 - val_accuracy: 0.4200\n",
            "Epoch 31/35\n",
            "54/54 [==============================] - 21s 301ms/step - loss: 1.1640 - accuracy: 0.4638 - val_loss: 1.1446 - val_accuracy: 0.4350\n",
            "Epoch 32/35\n",
            "54/54 [==============================] - 13s 241ms/step - loss: 1.1522 - accuracy: 0.4875 - val_loss: 1.2009 - val_accuracy: 0.4375\n",
            "Epoch 33/35\n",
            "54/54 [==============================] - 15s 281ms/step - loss: 1.1598 - accuracy: 0.4631 - val_loss: 1.0966 - val_accuracy: 0.5400\n",
            "Epoch 34/35\n",
            "54/54 [==============================] - 16s 294ms/step - loss: 1.1382 - accuracy: 0.4869 - val_loss: 1.1321 - val_accuracy: 0.4425\n",
            "Epoch 35/35\n",
            "54/54 [==============================] - 14s 253ms/step - loss: 1.2046 - accuracy: 0.4525 - val_loss: 1.1274 - val_accuracy: 0.5225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Creating Testing directories for Level 3 dms classification\n",
        "</div>"
      ],
      "metadata": {
        "id": "d43aCEvqBncE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "\n",
        "dalle_dir=\"/content/drive/MyDrive/diffusion_datasets/dalle/0_dalle\"\n",
        "glide_dir=\"/content/drive/MyDrive/diffusion_datasets/glide_100_27/1_glide\"\n",
        "guided_dir=\"/content/drive/MyDrive/diffusion_datasets/guided/2_guided\"\n",
        "ldm_dir=\"/content/drive/MyDrive/diffusion_datasets/ldm_200/3_ldm\"\n",
        "\n",
        "os.makedirs(\"/content/drive/MyDrive/testing_dataset_lvl3_dms\", exist_ok=True)\n",
        "\n",
        "\n",
        "testing_dir = '/content/drive/MyDrive/testing_dataset_lvl3_dms'\n",
        "\n",
        "\n",
        "dalle_images = [os.path.join(dalle_dir, filename) for filename in os.listdir(dalle_dir)][500:1000]\n",
        "glide_images = [os.path.join(glide_dir, filename) for filename in os.listdir(glide_dir)][500:1000]\n",
        "guided_images = [os.path.join(guided_dir, filename) for filename in os.listdir(guided_dir)][500:1000]\n",
        "ldm_images = [os.path.join(ldm_dir, filename) for filename in os.listdir(ldm_dir)][500:1000]\n",
        "\n",
        "all_images = dalle_images + glide_images + guided_images + ldm_images\n",
        "\n",
        "for image_path in all_images:\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "    destination_dir = os.path.join(testing_dir, class_name)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "    shutil.copy(image_path, destination_dir)\n"
      ],
      "metadata": {
        "id": "GK7E0Z92y7xy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <div class=\"markdown-google-sans\">\n",
        "\n",
        "## Testing the model for level 3 DMs classification\n",
        "</div>"
      ],
      "metadata": {
        "id": "i15NLsVyB2EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/saved_models_deepfake/lvl3_dalle.h5')\n",
        "\n",
        "test_dir = '/content/drive/MyDrive/testing_dataset_lvl3_dms'\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "_, test_accuracy = loaded_model.evaluate(test_generator)\n",
        "\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81k2Z6FL71sP",
        "outputId": "abb8b6d2-8909-4545-c5b5-634660c41843"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 4 classes.\n",
            "67/67 [==============================] - 252s 4s/step - loss: 1.1817 - accuracy: 0.4845\n",
            "Test accuracy: 0.484499990940094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bI8_fEDdA_c6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}